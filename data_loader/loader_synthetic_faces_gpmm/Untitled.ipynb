{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c1896f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that all ok\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Bernardo: Synthetic Faces GPMM dataset.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from math import floor\n",
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "#ROOT_DIR = BASE_DIR\n",
    "#sys.path.append(os.path.join(ROOT_DIR, '../../../utils'))\n",
    "#import provider\n",
    "import struct\n",
    "\n",
    "from data_loader.loader_synthetic_faces_gpmm. import TreeSyntheticFacesGPMM \n",
    "\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    # Bernardo\n",
    "    pc /= 100\n",
    "    pc = (pc - pc.min()) / (pc.max() - pc.min())\n",
    "\n",
    "    # l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "\n",
    "    return pc\n",
    "\n",
    "class SyntheticFacesGPMM_Dataset():\n",
    "    def __init__(self, root, batch_size = 32, npoints = 1024, num_classes=100, num_expressions=50, split='train', normalize=True, normal_channel=False, modelnet10=False, cache_size=15000, shuffle=None):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.npoints = npoints\n",
    "        self.normalize = normalize\n",
    "                \n",
    "        # Bernardo\n",
    "        pc_subjects_paths, unique_subjects_names = TreeSyntheticFacesGPMM().get_pointclouds_paths_with_subjects_names(dir_path=self.root, num_classes=num_classes, num_expressions=num_expressions)\n",
    "        # print('synthetic_faces_gpmm_dataset.py: TreeSyntheticFacesGPMM_Dataset(): __init__(): pc_subjects_paths =', pc_subjects_paths)\n",
    "        # print('unique_subjects_names:', unique_subjects_names)        \n",
    "\n",
    "        self.cat = unique_subjects_names    # Bernardo\n",
    "        self.classes = dict(zip(self.cat, range(len(self.cat))))  \n",
    "        self.num_classes = len(unique_subjects_names)\n",
    "        self.normal_channel = normal_channel\n",
    "        # print 'self.cat:', self.cat\n",
    "        # print 'self.classes:', self.classes\n",
    "\n",
    "        # Bernardo\n",
    "        assert(split=='train' or split=='test')\n",
    "        amount_train_samples_per_expr = int(floor(num_expressions * 0.8))\n",
    "        amount_test_samples_per_expr  = num_expressions - amount_train_samples_per_expr\n",
    "        self.datapath = []\n",
    "\n",
    "        if split=='train':            \n",
    "            for c in range(len(unique_subjects_names)):\n",
    "                #print('train indexes:', c*num_expressions, ':', c*num_expressions+amount_train_samples_per_expr)\n",
    "                self.datapath += pc_subjects_paths[c*num_expressions:c*num_expressions+amount_train_samples_per_expr]\n",
    "\n",
    "        elif split=='test':\n",
    "            for c in range(len(unique_subjects_names)):\n",
    "                # print('test indexes:', c*num_expressions+amount_train_samples_per_expr, ':', (c+1)*num_expressions)\n",
    "                self.datapath += pc_subjects_paths[c*num_expressions+amount_train_samples_per_expr:(c+1)*num_expressions]\n",
    "        \n",
    "        self.cache_size = cache_size # how many data points to cache in memory\n",
    "        self.cache = {} # from index to (point_set, cls) tuple\n",
    "\n",
    "        if shuffle is None:\n",
    "            if split == 'train': self.shuffle = True\n",
    "            else: self.shuffle = False\n",
    "        else:\n",
    "            self.shuffle = shuffle\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def _augment_batch_data(self, batch_data):\n",
    "        if self.normal_channel:\n",
    "            rotated_data = provider.rotate_point_cloud_with_normal(batch_data)\n",
    "            rotated_data = provider.rotate_perturbation_point_cloud_with_normal(rotated_data)\n",
    "        else:\n",
    "            rotated_data = provider.rotate_point_cloud(batch_data)\n",
    "            rotated_data = provider.rotate_perturbation_point_cloud(rotated_data)\n",
    "    \n",
    "        jittered_data = provider.random_scale_point_cloud(rotated_data[:,:,0:3])\n",
    "        jittered_data = provider.shift_point_cloud(jittered_data)\n",
    "        jittered_data = provider.jitter_point_cloud(jittered_data)\n",
    "        rotated_data[:,:,0:3] = jittered_data\n",
    "        return provider.shuffle_points(rotated_data)\n",
    "\n",
    "\n",
    "    def _readbcn(self, file):\n",
    "        npoints = os.path.getsize(file) // 4\n",
    "        with open(file,'rb') as f:\n",
    "            raw_data = struct.unpack('f'*npoints,f.read(npoints*4))\n",
    "            data = np.asarray(raw_data,dtype=np.float32)       \n",
    "        # data = data.reshape(7, len(data)//7)   # original\n",
    "        data = data.reshape(3, len(data)//3).T   # Bernardo\n",
    "        return data                        # Bernardo    \n",
    "\n",
    "\n",
    "    def _get_item(self, index): \n",
    "        if index in self.cache:\n",
    "            point_set, cls = self.cache[index]\n",
    "        else:\n",
    "            fn = self.datapath[index]\n",
    "            cls = self.classes[self.datapath[index][0]]\n",
    "            cls = np.array([cls]).astype(np.int32)\n",
    "\n",
    "            # Bernardo\n",
    "            print('synthetic_faces_gpmm_dataset.py: _get_item(): loading file:', fn[1])\n",
    "\n",
    "            # point_set = np.loadtxt(fn[1],delimiter=',').astype(np.float32)   # original\n",
    "            # point_set = np.load(fn[1]).astype(np.float32)                    # Bernardo\n",
    "            point_set = self._readbcn(fn[1]).astype(np.float32)                 # Bernardo\n",
    "\n",
    "            # Bernardo\n",
    "            if point_set.shape[1] == 7:        # if contains curvature\n",
    "                point_set = point_set[:,:-1]   # remove curvature column\n",
    "\n",
    "            # Take the first npoints\n",
    "            point_set = point_set[0:self.npoints,:]\n",
    "            if self.normalize:\n",
    "                point_set[:,0:3] = pc_normalize(point_set[:,0:3])\n",
    "            if not self.normal_channel:\n",
    "                point_set = point_set[:,0:3]\n",
    "            if len(self.cache) < self.cache_size:\n",
    "                self.cache[index] = (point_set, cls)\n",
    "        return point_set, cls\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self._get_item(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "    def num_channel(self):\n",
    "        if self.normal_channel:\n",
    "            return 6\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def reset(self):\n",
    "        self.idxs = np.arange(0, len(self.datapath))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "        self.num_batches = (len(self.datapath)+self.batch_size-1) // self.batch_size\n",
    "        self.batch_idx = 0\n",
    "\n",
    "    def has_next_batch(self):\n",
    "        return self.batch_idx < self.num_batches\n",
    "\n",
    "    def next_batch(self, augment=False):\n",
    "        #returned dimension may be smaller than self.batch_size\n",
    "        start_idx = self.batch_idx * self.batch_size\n",
    "        end_idx = min((self.batch_idx+1) * self.batch_size, len(self.datapath))\n",
    "        bsize = end_idx - start_idx\n",
    "        batch_data = np.zeros((bsize, self.npoints, self.num_channel()))\n",
    "        batch_label = np.zeros((bsize), dtype=np.int32)\n",
    "        for i in range(bsize):\n",
    "            ps,cls = self._get_item(self.idxs[i+start_idx])\n",
    "            batch_data[i] = ps\n",
    "            batch_label[i] = cls\n",
    "        self.batch_idx += 1\n",
    "        if augment: batch_data = self._augment_batch_data(batch_data)\n",
    "        return batch_data, batch_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8099d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test dataset\n",
      "1000\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000051/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000051/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000143/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000143/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000156/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000156/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000318/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000318/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000358/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000358/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000559/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000559/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000696/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000696/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000925/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000925/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000992/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400000992/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001005/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001005/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001011/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001011/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001213/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001213/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001216/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001216/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001254/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001254/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001287/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001287/009.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001460/008.bc\n",
      "synthetic_faces_gpmm_dataset.py: _get_item(): loading file: /datasets1/bjgbiesseck/SyntheticFacesGPMM/400001460/009.bc\n",
      "(32, 28588, 3)\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pickle\n",
    "from tree_synthetic_faces import TreeSyntheticFacesGPMM \n",
    "\n",
    "\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "DATA_PATH = '/datasets1/bjgbiesseck/SyntheticFacesGPMM'\n",
    "NUM_POINT = 28588\n",
    "n_classes = 100\n",
    "n_expressions = 10\n",
    "print(\"Train and Test dataset\")\n",
    "TRAIN_DATASET = SyntheticFacesGPMM_Dataset(root=DATA_PATH, npoints=NUM_POINT, num_classes=n_classes, num_expressions=n_expressions, split='train', normal_channel=False, batch_size=32)\n",
    "TEST_DATASET  = SyntheticFacesGPMM_Dataset(root=DATA_PATH, npoints=NUM_POINT, num_classes=n_classes, num_expressions=n_expressions, split='test', normal_channel=False, batch_size=32)\n",
    "\n",
    "\n",
    "pc_subjects_paths, unique_subjects_names = TreeSyntheticFacesGPMM().get_pointclouds_paths_with_subjects_names(DATA_PATH, num_classes=n_classes, num_expressions=n_expressions)\n",
    "\n",
    "\n",
    "print(len(pc_subjects_paths))\n",
    "\n",
    "batch_data, batch_label = TEST_DATASET.next_batch(augment=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pt = batch_data\n",
    "#pt = pt.numpy()\n",
    "\n",
    "print(pt.shape)\n",
    "\n",
    "# Save the numpy array to disk\n",
    "print(np.save('/home/pbqv20/PointNeXt/modelNet3dPointCloud/', pt))\n",
    "\n",
    "print(os.path.exists('/home/pbqv20/PointNeXt/modelNet3dPointCloud'))\n",
    "\n",
    "# Open the file in binary mode and use pickle.dump() to save the array to disk\n",
    "with open('/home/pbqv20/PointNeXt/modelNet3dPointCloud/pc2.npy', 'wb') as file:\n",
    "    pickle.dump(pt, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0c07e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'provider'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msynthetic_faces_gpmm_dataset\u001b[39;00m  \n",
      "File \u001b[0;32m~/PointNeXt/data_loader/loader_synthetic_faces_gpmm/synthetic_faces_gpmm_dataset.py:16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#BASE_DIR = os.path.dirname(os.path.abspath(__file__))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#ROOT_DIR = BASE_DIR\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#sys.path.append(os.path.join(ROOT_DIR, '../../../utils'))\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mprovider\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstruct\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtree_synthetic_faces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreeSyntheticFacesGPMM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'provider'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c870af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
